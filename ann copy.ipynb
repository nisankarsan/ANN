{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lP6JLo1tGNBg"
   },
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWZyYmS_UE_L"
   },
   "source": [
    "### Importing the libraries !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices())\n",
    "print(tf.version.VERSION) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1E0Q3aoKUCRX"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKWAkFVGUU0Z"
   },
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- foor X we don't need to take rownumber, costurmer id and surname, so we only takes the column starting from creditscore to except eestimated salary\n",
    "- for X so we use 3:-1 to do this formation it takes starting 3.columun (0 index for python) then not included last columun which we udes -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/nisankarsan/Downloads/Codes/Deep Learning A-Z/Part 1 - Artificial Neural Networks (ANN)/Churn_Modelling.csv')\n",
    "x = dataset.iloc[:,3:-1].values #all rows, all columns except the last one which is the target variable (Exited)\n",
    "y = dataset.iloc[:,-1].values #dependent variable vector (all rows, only the last column) whicc is the target variable (Exited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the 1 below is the first costumer row which is stay in the bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6bQ0UgSU-NJ"
   },
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 2 categorical variables in the dataset \n",
    "1. the country of residence of the costumers(which is 2.column of X)\n",
    "2. gender of the costumer( which is 3.column of X)\n",
    "\n",
    "we'll have to so some encoding work here to encode these categorical data in either simple labels, 0 or 1 for the gender or some hot encoding for this categorical which indeed there is o relationship order between these values, between the categories France, Spain ang Germany "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "le5MJreAbW52"
   },
   "source": [
    "Label Encoding the \"Gender\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder() \n",
    "#object of the class LabelEncoder which is used to encode the categorical variable into numerical variable\n",
    "x[:,2] = le.fit_transform(x[:,2])  #3.column but python is 0 based so it is the 2nd column\n",
    "#encoding the gender column into numerical values (0,1) using the fit_transform method of the LabelEncoder class object le \n",
    "#takes all the rows of the 3rd column of the x matrix and encodes them into numerical values and replaces them in the x matrix itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is relationship with female and male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 0 ... 1 1 101348.88]\n",
      " [608 'Spain' 0 ... 0 1 112542.58]\n",
      " [502 'France' 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 0 ... 0 1 42085.58]\n",
      " [772 'Germany' 1 ... 1 0 92888.52]\n",
      " [792 'France' 0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(x) #0 for Female, 1 for Male"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUxGZezpbMcb"
   },
   "source": [
    "One Hot Encoding the \"Geography\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but there is no relationsship between the countries, so we do hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "# the countries in the 1st column are encoded into 3 columns using the OneHotEncoder class\n",
    "X = np.array(ct.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 ... 1 1 101348.88]\n",
      " [0.0 0.0 1.0 ... 0 1 112542.58]\n",
      " [1.0 0.0 0.0 ... 1 0 113931.57]\n",
      " ...\n",
      " [1.0 0.0 0.0 ... 0 1 42085.58]\n",
      " [0.0 1.0 0.0 ... 1 0 92888.52]\n",
      " [1.0 0.0 0.0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first column is France, \\\n",
    "the second column is Germany and \\\n",
    "the third column is Spain \\\n",
    "(0,0,1) for Spain, \\\n",
    "(0,1,0) for Germany \\\n",
    "(1,0,0) for France "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHol938cW8zd"
   },
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "#splitting the dataset into training and testing set using the train_test_split method of the model_selection class of the sklearn library\n",
    "#X_train is the training set of the independent variables\n",
    "#X_test is the testing set of the independent variables\n",
    "#y_train is the training set of the dependent variable\n",
    "#y_test is the testing set of the dependent variable\n",
    "#test_size=0.2 means 20% of the dataset is used for testing and 80% is used for training\n",
    "#random_state=0 is used to get the same output as the instructor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE_FcHyfV3TQ"
   },
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when you build ANN you have to apply feature scaling, we apply feature scaling to all out features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling is critical to ensure accurate outcomes, faster computations, and to prevent any single feature from dominating the model due to its range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "#scaling the independent variables using the StandardScaler class of the preprocessing module of the sklearn library\n",
    "#fit_transform method is used to fit the StandardScaler object to the training set and transform it\n",
    "#transform method is used to transform the testing set using the same object that was fitted to the training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zfEzkRVXIwF"
   },
   "source": [
    "## Part 2 - Building the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvdeScabXtlB"
   },
   "source": [
    "### Initializing the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the certain class is sequential class which allows exactly to buil an artificial neural network but as a sequence of  layers as opposed to computational graph\n",
    "- start initial layer then have the fully connected layers up to the final output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 19:42:15.385962: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-11-08 19:42:15.385994: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-11-08 19:42:15.386017: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-11-08 19:42:15.386097: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-08 19:42:15.386143: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "ann = tf.keras.models.Sequential() #object of the Sequential class of the keras module of the tensorflow library\n",
    "#used to initialize the neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP6urV6SX7kS"
   },
   "source": [
    "### Adding the input layer and the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the first hidden layer of the neural network\n",
    "# Dense class of the layers module of the keras library is used to add the input layer and the first hidden layer\n",
    "# units=6 is the number of neurons in the first hidden layer which\n",
    "# activation='relu' is the activation function used in the first hidden layer which\n",
    "# is the rectifier function (relu) because it is the first hidden layer and the input layer\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BELWAc_8YJze"
   },
   "source": [
    "### Adding the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyNEe6RXYcU4"
   },
   "source": [
    "### Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "# Adding the output layer of the neural network\n",
    "# units=1 is the number of neurons in the output layer \n",
    "# which is 1 because it is a binary classification problem (0 or 1) \n",
    "# activation='sigmoid' is the activation function used in the output layer\n",
    "# which is the sigmoid function because it is a binary classification problem (0 or 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT4u2S1_Y4WG"
   },
   "source": [
    "## Part 3 - Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GWlJChhY_ZI"
   },
   "source": [
    "### Compiling the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compile ANN with an optimizer, a loss function, and a metric which accuracy because we're doing some classification. \n",
    "-  the default is Adam Optimizer, which is a very performance optimizer that can perform the Stochastic Gradient Descent. \n",
    "\n",
    "Stochastic Gradient Descent allow to do:\n",
    "- update the weights, \n",
    "reduce the cost function error between your predictions and real result. Each iteration compare the predictions in a batch to the real results in the same batch. And that the optimizer here will update the weight through Stochastic Gradien descent to at the next iteration, hopefully reduce the loss.\n",
    "\n",
    "What is batch size? The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "\n",
    "loss function which is the way to compute the difference between the predictions and the real results, and the accuracy of course beacuse that's our final evaluation metrics \n",
    "\n",
    "For loss function:\n",
    "- if you're doing binary classification, you know classification when you have to predict a binary outcome so the lost function always be the following one entered in quotes which is \"binary_crossentropy\" and activation sigmoid.\n",
    "- if you're doing non-binary classification, predicting 3 different categries, you use \"category called cross entropy loss\". activation shouldn't be sigmoid but sofmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics =['accuracy'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QR_G5u7ZLSM"
   },
   "source": [
    "### Training the ANN on the Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How starting training ?\n",
    "- take out ANN and then call a new method which will perform the training and enter couple of parameters\n",
    "- the fit method takes always the same parameters, first X_train which is matrix of features of the training set.\n",
    "- Y_train for the dependent variable vector of the training set.\n",
    "- batch size, batch learning is always more efficient and more performant when training an artificial neural network, meaning that instead of comparing your prediction to the real result, one by one to compute and reduce the loss, gonna do that with several predictions compared to several results into a batch. And the batch size parameter gives exactly the number of predictions you want to have in the batch to be compared to that same number of result. And the classic value of the batch size that usually chosen 32, if you don't want to spend too much time tuning this hyper parameter, choose default value 32\n",
    "\n",
    "- the number of epochs: the neural network has to be trained over certain amount of epochs so as to improve the accuracy over time.  Don't choose samll epochs because neural network needs a certain amount of epochs in order to learn properly, learn the correlations to get ultimate best predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 19:42:35.040294: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 3s 3ms/step - loss: 0.7047 - accuracy: 0.6386\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4707 - accuracy: 0.7889\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4406 - accuracy: 0.8059\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4344 - accuracy: 0.8066\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4320 - accuracy: 0.8085\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4320 - accuracy: 0.8070\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4318 - accuracy: 0.8081\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4321 - accuracy: 0.8081\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4325 - accuracy: 0.8087\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4334 - accuracy: 0.8064\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4335 - accuracy: 0.8077\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4345 - accuracy: 0.8061\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4357 - accuracy: 0.8055\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4360 - accuracy: 0.8046\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4400 - accuracy: 0.8050\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.4389 - accuracy: 0.8039\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4397 - accuracy: 0.8054\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4404 - accuracy: 0.8051\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4419 - accuracy: 0.8050\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4412 - accuracy: 0.8060\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4423 - accuracy: 0.8033\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4422 - accuracy: 0.8070\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4422 - accuracy: 0.8071\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4429 - accuracy: 0.8037\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4488 - accuracy: 0.8029\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4437 - accuracy: 0.8030\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4434 - accuracy: 0.8067\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4498 - accuracy: 0.8064\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4458 - accuracy: 0.8058\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4482 - accuracy: 0.8052\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4431 - accuracy: 0.8045\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4497 - accuracy: 0.8064\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4578 - accuracy: 0.8066\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4470 - accuracy: 0.8076\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4503 - accuracy: 0.8014\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4516 - accuracy: 0.8041\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4530 - accuracy: 0.8044\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4465 - accuracy: 0.8045\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4454 - accuracy: 0.8034\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4480 - accuracy: 0.8075\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4620 - accuracy: 0.8074\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4674 - accuracy: 0.8044\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4585 - accuracy: 0.8019\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4543 - accuracy: 0.8050\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4681 - accuracy: 0.8021\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4565 - accuracy: 0.8066\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4657 - accuracy: 0.8027\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4656 - accuracy: 0.8011\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4573 - accuracy: 0.8048\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4682 - accuracy: 0.8054\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4653 - accuracy: 0.7993\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4607 - accuracy: 0.8056\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4529 - accuracy: 0.8064\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4613 - accuracy: 0.8041\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4657 - accuracy: 0.8021\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4670 - accuracy: 0.8027\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4720 - accuracy: 0.8012\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4702 - accuracy: 0.8006\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4767 - accuracy: 0.7993\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4727 - accuracy: 0.8006\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4718 - accuracy: 0.7980\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4712 - accuracy: 0.7984\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4861 - accuracy: 0.8015\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4848 - accuracy: 0.8021\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4724 - accuracy: 0.8046\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4794 - accuracy: 0.8030\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4767 - accuracy: 0.7997\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4594 - accuracy: 0.8030\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4932 - accuracy: 0.7983\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4760 - accuracy: 0.8051\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4742 - accuracy: 0.8010\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5001 - accuracy: 0.7983\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4855 - accuracy: 0.8010\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4716 - accuracy: 0.8045\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4789 - accuracy: 0.8006\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4706 - accuracy: 0.8009\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4767 - accuracy: 0.8034\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4800 - accuracy: 0.8010\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4657 - accuracy: 0.8025\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5068 - accuracy: 0.7974\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4905 - accuracy: 0.7997\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5024 - accuracy: 0.7997\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5090 - accuracy: 0.7939\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4722 - accuracy: 0.8061\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4840 - accuracy: 0.7994\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4836 - accuracy: 0.8021\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5103 - accuracy: 0.8024\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4798 - accuracy: 0.7966\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5027 - accuracy: 0.7983\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 0.4638 - accuracy: 0.8029\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7893\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4896 - accuracy: 0.8000\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4851 - accuracy: 0.8031\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4746 - accuracy: 0.8040\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4896 - accuracy: 0.8005\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4873 - accuracy: 0.8026\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.4873 - accuracy: 0.8015\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5002 - accuracy: 0.7933\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5235 - accuracy: 0.7922\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 1s 3ms/step - loss: 0.5151 - accuracy: 0.7985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3022cda00>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train, y_train, batch_size=32, epochs=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJj5k2MxZga3"
   },
   "source": [
    "## Part 4 - Making the predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84QFoqGYeXHL"
   },
   "source": [
    "### Predicting the result of a single observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sc is the object of the StandardScaler class which is used to scale the independent variables\n",
    "\n",
    "- do not forget to put [[]] that any input of the predict method must be a 2D array. Whether it is the test set to predict an ensemble of outcomes for an ensemble of observations or whether it is a single prediction, well anything has to be in a double pair of square brackets [[]] which therefore makes a 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "#Predicting result for Single Observation\n",
    "print(ann.predict(sc.transform([[1, 0, 0, 1,600, 40, 3, 60000, 2, 1, 1,50000]])) > 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGRo3eacgDdC"
   },
   "source": [
    "**Homework**\n",
    "\n",
    "Use our ANN model to predict if the customer with the following informations will leave the bank:\n",
    "\n",
    "Geography: France\n",
    "\n",
    "Credit Score: 600\n",
    "\n",
    "Gender: Male\n",
    "\n",
    "Age: 40 years old\n",
    "\n",
    "Tenure: 3 years\n",
    "\n",
    "Balance: \\$ 60000\n",
    "\n",
    "Number of Products: 2\n",
    "\n",
    "Does this customer have a credit card? Yes\n",
    "\n",
    "Is this customer an Active Member: Yes\n",
    "\n",
    "Estimated Salary: \\$ 50000\n",
    "\n",
    "So, should we say goodbye to that customer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhU1LTgPg-kH"
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGjx94g2n7OV"
   },
   "source": [
    "Therefore, our ANN model predicts that this customer stays in the bank!\n",
    "\n",
    "**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array.\n",
    "\n",
    "**Important note 2:** Notice also that the \"France\" country was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the first row of the matrix of features X, \"France\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, because the dummy variables are always created in the first columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7yx47jPZt11"
   },
   "source": [
    "### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0oyfLWoaEGw"
   },
   "source": [
    "### Making the Confusion Matrix"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP/7bUEMU+I65CO8Na8198Y",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
